---
title: BCC analysis on youtube factual recommendations
subtitle: our analysis and suggestion for the next step
date: 2019-09-13T13:01:21+01:00
draft: false
description: feedback on - YouTube's algorithm promotes fake cancer cures in a number of languages, a BBC investigation has found.
type: post
---

## Here we report some portions form this great BCC investigation: [YouTube advertises big brands alongside fake cancer cure videos](https://www.bbc.com/news/blogs-trending-49483681), By Flora Carmichael & Juliana Gragnani, on a section called "Beyond Fake News & BBC Monitoring".

    Searching YouTube across 10 languages, the BBC found more than 80 videos containing health misinformation - mainly bogus cancer cures. Ten of the videos found had more than a million views. Many were accompanied by adverts.


## The analysis of BCC consists of looking at what YouTube suggests and do qualitative analysis. The approach has some pro: you judge with your own knowledge instead of relying on automated decision making. Has some cons: you might run a certain amount of limited test, and they will have some bias due to the parameters outside of your control. Such as: are you using a VPN? Or the BCC office IP address? Is the browser clean or is your personal profile?

## I'm a component of tracking.exposed collaborative project. I want to show some findings of ours, which might be interesting to compare with your methodology.

    YouTube's advertising system means that both the Google-owned company and the video makers are making money from the misleading clips.

## Note: we don't look yet at advertising. (and might not be done ASAP.), we focus on recommendations algorithm

    Shut down in English - but not other languages

    In January, YouTube announced they would be "reducing recommendations of borderline content and content that could misinform users in harmful ways—such as videos promoting a phony miracle cure for a serious illness."

    But the company said the change would initially only affect recommendations of a very small set of videos in the United States, and does not apply in languages other than English.

## This (a.k.a. digital colonialism) is an indeed real effect, and because of the targeting capability, not just by language is affected by the _protection_ which Google my offer, but also the attack of hyper distorted algorithmic recommendations. A very selected demographic in particular regional context might receive suggested content we wouldn't. As a researcher, I can easily be myself (by using my browser) or an anonymous one — only two simulated conditions over billions of different profiles. The unique footprint of the user is a resource only Google has. Collaboratively we can see with the eyes of other users, and better analyze the algorithm.

    The BBC search covered English, Portuguese, Russian, Arabic, Persian, Hindi, German, Ukrainian, French and Italian.

    Erin McAweeney, a research analyst at the Data & Society institute, explained that because YouTube's algorithm recommends similar videos to the ones you have just watched, it is continuously "carving a path" from one video to the next, regardless of the credibility of the advice offered within.

    "Someone can start out on a credible video and be suggested to watch a juice cure video next. A recommendation system doesn't know credible from non-credible content." McAweeney says.

    YouTube has stated that its recommendation system - which has been accused of leading users down rabbit holes of conspiracy theories and radicalisation - would change, recommending videos that are credible and trustworthy to people that are watching videos that might not be.
